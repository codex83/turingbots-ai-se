{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Git Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs_folder = 'gs://msca-bdp-data-open/final_project_git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data size in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total directory size: 1.36 TiB     gs://msca-bdp-data-open/final_project_git\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = 'gsutil du -s -h ' + gcs_folder\n",
    "\n",
    "p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "for line in p.stdout.readlines():\n",
    "    print (f'Total directory size: {line}')\n",
    "    \n",
    "retval = p.wait() # Wait for the child process to terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Git data from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Languages\n",
    "Programming languages by repository as reported by GitHub's https://developer.github.com/v3/repos/#list-languages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records read from dataframe *languages*: 3,325,634\n",
      "CPU times: user 13.7 ms, sys: 556 Âµs, total: 14.3 ms\n",
      "Wall time: 11.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time   \n",
    "    \n",
    "df_languages = spark.read.parquet(os.path.join(gcs_folder, 'languages'))\n",
    "print(f'Records read from dataframe *languages*: {df_languages.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_languages.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|          repo_name|    language|\n",
      "+-------------------+------------+\n",
      "|  lemi136/puntovent|   [{C, 80}]|\n",
      "|     taxigps/nctool| [{C, 4461}]|\n",
      "|        ahy1/strbuf| [{C, 5573}]|\n",
      "|nleiten/mod_rpaf-ng|[{C, 30330}]|\n",
      "|kmcallister/alameda|[{C, 17077}]|\n",
      "+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_languages.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licenses\n",
    "Open source license SPDX code for each repository as detected by https://developer.github.com/v3/licenses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records read from dataframe *licenses*: 3,325,634\n",
      "CPU times: user 0 ns, sys: 6.39 ms, total: 6.39 ms\n",
      "Wall time: 1.84 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time   \n",
    "    \n",
    "df_licenses = spark.read.parquet(os.path.join(gcs_folder, 'licenses'))\n",
    "print(f'Records read from dataframe *licenses*: {df_licenses.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_licenses.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|           repo_name|     license|\n",
      "+--------------------+------------+\n",
      "|autarch/Dist-Zill...|artistic-2.0|\n",
      "|thundergnat/Prime...|artistic-2.0|\n",
      "|kusha-b-k/Turabia...|artistic-2.0|\n",
      "|onlinepremiumoutl...|artistic-2.0|\n",
      "|huangyuanlove/Lia...|artistic-2.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_licenses.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commits\n",
    "Unique Git commits from open source repositories on GitHub, pre-grouped by repositories they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>(3483 + 1) / 3484]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records read from dataframe *commits*: 265,419,190\n",
      "CPU times: user 350 ms, sys: 73.5 ms, total: 423 ms\n",
      "Wall time: 1min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time   \n",
    "    \n",
    "df_commits = spark.read.parquet(os.path.join(gcs_folder, 'commits'))\n",
    "print(f'Records read from dataframe *commits*: {df_commits.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- commit: string (nullable = true)\n",
      " |-- tree: string (nullable = true)\n",
      " |-- parent: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- author: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- time_sec: long (nullable = true)\n",
      " |    |-- tz_offset: long (nullable = true)\n",
      " |    |-- date: struct (nullable = true)\n",
      " |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |-- nanos: long (nullable = true)\n",
      " |-- committer: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- time_sec: long (nullable = true)\n",
      " |    |-- tz_offset: long (nullable = true)\n",
      " |    |-- date: struct (nullable = true)\n",
      " |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |-- nanos: long (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- trailer: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |-- difference: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- old_mode: long (nullable = true)\n",
      " |    |    |-- new_mode: long (nullable = true)\n",
      " |    |    |-- old_path: string (nullable = true)\n",
      " |    |    |-- new_path: string (nullable = true)\n",
      " |    |    |-- old_sha1: string (nullable = true)\n",
      " |    |    |-- new_sha1: string (nullable = true)\n",
      " |    |    |-- old_repo: string (nullable = true)\n",
      " |    |    |-- new_repo: string (nullable = true)\n",
      " |-- difference_truncated: boolean (nullable = true)\n",
      " |-- repo_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- encoding: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_commits.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------+\n",
      "|              commit|                tree|              parent|              author|           committer|             subject|             message|             trailer|difference|difference_truncated|           repo_name|encoding|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------+\n",
      "|aa358905a1b12c6fa...|df3f8bf61bf1cb0df...|[ea230a45a0e97e4d...|{conda-forge-coor...|{conda-forge-coor...|Updated the qceng...|Updated the qceng...|                  []|        []|                true|[conda-forge/feed...|    NULL|\n",
      "|5a6b6d6d29489f858...|ff89accb7e283ca88...|[4ee369feb64ee97d...|{Rob Allen, 7e09c...|{Rob Allen, 7e09c...|Merge remote-trac...|Merge remote-trac...|                  []|        []|                NULL|[MadCat34/zend-es...|    NULL|\n",
      "|6b6ac3b8ab7363b22...|915acc1689313e3e2...|[f10bea38c15c335e...|{Zhihui Zhang, 96...|{Zhihui Zhang, 96...|provide hook to o...|provide hook to o...|[{git-svn-id, f2a...|        []|                NULL|[pscedu/slash2-st...|    NULL|\n",
      "|e26e1f63938b983ce...|a86ea389f72e12b89...|[e263e5fcd2ced2e2...|{conda-forge-coor...|{conda-forge-coor...|Updated the mailc...|Updated the mailc...|                  []|        []|                true|[conda-forge/feed...|    NULL|\n",
      "|2a896010ccf1c86c2...|ba238795c2befc08b...|[721364dbb5d1515f...|{armaneshaghi, f6...|{armaneshaghi, f6...|    2014-03-06T04:30|  2014-03-06T04:30\\n|                  []|        []|                NULL|[armaneshaghi/pro...|    NULL|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_commits.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "Unique file contents of text files under 1 MiB on the HEAD branch.  \n",
    "Can be joined to `files` dataset using the id columns to identify the repository and file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===================================================>(6980 + 1) / 6981]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records read from dataframe *commits*: 281,191,977\n",
      "CPU times: user 461 ms, sys: 134 ms, total: 595 ms\n",
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time   \n",
    "    \n",
    "df_contents = spark.read.parquet(os.path.join(gcs_folder, 'contents'))\n",
    "print(f'Records read from dataframe *commits*: {df_contents.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- size: long (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- binary: boolean (nullable = true)\n",
      " |-- copies: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_contents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------+------+\n",
      "|                  id| size|             content|binary|copies|\n",
      "+--------------------+-----+--------------------+------+------+\n",
      "|d5b1049fdaa182fa5...| 1570|{\"version\":3,\"sou...| false|   256|\n",
      "|896830f9ea31efd6b...|18616|                NULL|  true|     1|\n",
      "|bf1e2a8490344601c...|15580|                NULL|  true|     1|\n",
      "|e5976431eba91aa73...| 3328|                NULL|  true|     1|\n",
      "|311532e41682cab22...| 8970|                NULL|  true|     1|\n",
      "+--------------------+-----+--------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_contents.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files\n",
    "File metadata for all files at HEAD.  \n",
    "Join with `contents` dataset on id columns to search text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>(1079 + 1) / 1080]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records read from dataframe *files*: 2,309,424,945\n",
      "CPU times: user 88.1 ms, sys: 26 ms, total: 114 ms\n",
      "Wall time: 31.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time   \n",
    "    \n",
    "df_files = spark.read.parquet(os.path.join(gcs_folder, 'files'))\n",
    "print(f'Records read from dataframe *files*: {df_files.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- ref: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- symlink_target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_files.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+-----+--------------------+--------------+\n",
      "|           repo_name|              ref|                path| mode|                  id|symlink_target|\n",
      "+--------------------+-----------------+--------------------+-----+--------------------+--------------+\n",
      "|    enzbang/diouzhtu|refs/heads/master|gwiad_wiki_servic...|33261|49365044eed287691...|          NULL|\n",
      "|TheMrNomis/Latex-...|refs/heads/master|             LFM.php|33261|ef8cb78feed7f2111...|          NULL|\n",
      "|TheMrNomis/Latex-...|refs/heads/master|PHP/LatexFlavored...|33261|d989ce59652f57efa...|          NULL|\n",
      "|    xurigan/uexJPush|refs/heads/master|EUExJPush/EUExJPu...|33261|85268b90caa19efa2...|          NULL|\n",
      "|    xurigan/uexJPush|refs/heads/master|EUExJPush/uexJPus...|33261|e1623bb9d8dc7db60...|          NULL|\n",
      "+--------------------+-----------------+--------------------+-----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_files.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:===================================================>(3483 + 1) / 3484]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+----------+--------------------+---------+--------+\n",
      "|   commit|     tree|   parent|   author|committer|  subject|  message|  trailer|difference|difference_truncated|repo_name|encoding|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+----------+--------------------+---------+--------+\n",
      "|265419190|265419190|265419190|265419190|265419190|265419190|265419190|265419190| 265419190|              220943|265419190|  126182|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+----------+--------------------+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, count\n",
    "\n",
    "# Check for nulls\n",
    "df_commits.select([count(col(c)).alias(c) for c in df_commits.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing values in Languages ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|repo_name|language|\n",
      "+---------+--------+\n",
      "|        0|       0|\n",
      "+---------+--------+\n",
      "\n",
      "--- Missing values in Commits ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+------+------+\n",
      "| id|size| content|binary|copies|\n",
      "+---+----+--------+------+------+\n",
      "|  0|   0|53037932|     0|     0|\n",
      "+---+----+--------+------+------+\n",
      "\n",
      "--- Missing values in Files ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+----+---+--------------+\n",
      "|repo_name|ref|path|mode| id|symlink_target|\n",
      "+---------+---+----+----+---+--------------+\n",
      "|        0|  0|   0|   0|  0|    2304200645|\n",
      "+---------+---+----+----+---+--------------+\n",
      "\n",
      "--- Missing values in Licenses ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|repo_name|license|\n",
      "+---------+-------+\n",
      "|        0|      0|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for name, df in [('Languages', df_languages), ('Commits', df_commits), ('Contents', df_contents), ('Files', df_files), ('Licenses', df_licenses)]:\n",
    "    print(f'--- Missing values in {name} ---')\n",
    "    df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue, 11 March 2025 03:48:12'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-runtime-0000253ceb24",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "htj2-ads-bdp on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000253ceb24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
