{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Git Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_folder = 'gs://msca-bdp-data-open/final_project_git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data size in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'gsutil du -s -h ' + gcs_folder\n",
    "\n",
    "p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "for line in p.stdout.readlines():\n",
    "    print (f'Total directory size: {line}')\n",
    "    \n",
    "retval = p.wait() # Wait for the child process to terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Git data from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Languages\n",
    "Programming languages by repository as reported by GitHub's https://developer.github.com/v3/repos/#list-languages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "    \n",
    "df_languages = spark.read.parquet(os.path.join(gcs_folder, 'languages'))\n",
    "print(f'Records read from dataframe *languages*: {df_languages.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_languages.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licenses\n",
    "Open source license SPDX code for each repository as detected by https://developer.github.com/v3/licenses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "    \n",
    "df_licenses = spark.read.parquet(os.path.join(gcs_folder, 'licenses'))\n",
    "print(f'Records read from dataframe *licenses*: {df_licenses.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_licenses.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commits\n",
    "Unique Git commits from open source repositories on GitHub, pre-grouped by repositories they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "    \n",
    "df_commits = spark.read.parquet(os.path.join(gcs_folder, 'commits'))\n",
    "print(f'Records read from dataframe *commits*: {df_commits.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commits.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "Unique file contents of text files under 1 MiB on the HEAD branch.  \n",
    "Can be joined to `files` dataset using the id columns to identify the repository and file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "    \n",
    "df_contents = spark.read.parquet(os.path.join(gcs_folder, 'contents'))\n",
    "print(f'Records read from dataframe *commits*: {df_contents.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contents.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files\n",
    "File metadata for all files at HEAD.  \n",
    "Join with `contents` dataset on id columns to search text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "    \n",
    "df_files = spark.read.parquet(os.path.join(gcs_folder, 'files'))\n",
    "print(f'Records read from dataframe *files*: {df_files.count():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the timeline of the data?\n",
    "\n",
    "#### Identify peaks and valleys in commit activity.\n",
    "#### Detect data gaps and unexpected spikes.\n",
    "#### Use visualization (e.g., Matplotlib, Seaborn, Plotly, Databricks display function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert timestamps and aggregate data over time\n",
    "commits_df = commits_df.withColumn(\"date\", col(\"commit_date\").cast(\"date\"))\n",
    "timeline_df = commits_df.groupBy(\"date\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(timeline_df[\"date\"], timeline_df[\"count\"], label=\"Commits Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Commits\")\n",
    "plt.title(\"GitHub Commits Timeline\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular programming languages on GitHub\n",
    "\n",
    "#### Analyze the frequency of different languages from the Languages dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_df = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/languages\")\n",
    "top_languages_df = languages_df.groupBy(\"language\").sum(\"bytes\").orderBy(\"sum(bytes)\", ascending=False)\n",
    "top_languages_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trending Programming Languages Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, sum\n",
    "\n",
    "# Load languages data\n",
    "languages_df = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/languages\")\n",
    "\n",
    "# Extract year from the repository timestamps\n",
    "languages_df = languages_df.withColumn(\"year\", year(col(\"timestamp\")))\n",
    "\n",
    "# Aggregate language usage by year\n",
    "language_trend_df = (\n",
    "    languages_df.groupBy(\"year\", \"language\")\n",
    "    .agg(sum(\"bytes\").alias(\"total_bytes\"))\n",
    "    .orderBy(\"year\", \"total_bytes\", ascending=[True, False])\n",
    ")\n",
    "\n",
    "# Show trends\n",
    "language_trend_df.show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Repositories (Commit Volume, Stars, Forks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Load commits data\n",
    "commits_df = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/commits\")\n",
    "\n",
    "# Count commits per repository\n",
    "repo_popularity_df = (\n",
    "    commits_df.groupBy(\"repo_name\")\n",
    "    .agg(count(\"*\").alias(\"commit_count\"))\n",
    "    .orderBy(col(\"commit_count\").desc())\n",
    ")\n",
    "\n",
    "# Show top repositories by commit volume\n",
    "repo_popularity_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying AI & Data Science Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# Define AI-related keywords\n",
    "ai_keywords = [\"machine learning\", \"deep learning\", \"pytorch\", \"tensorflow\", \"scikit-learn\", \"ml\", \"ai\"]\n",
    "\n",
    "# Filter repositories with AI keywords in their name or description\n",
    "ai_repos_df = (\n",
    "    commits_df.filter(\n",
    "        col(\"repo_name\").rlike(\"|\".join(ai_keywords)) | col(\"message\").rlike(\"|\".join(ai_keywords))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show AI-related repositories\n",
    "ai_repos_df.select(\"repo_name\", \"message\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Commit Reasons (NLP Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Load commit messages\n",
    "commit_msgs_df = commits_df.select(\"message\").dropna()\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(commit_msgs_df)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_words = remover.transform(words_data)\n",
    "\n",
    "# Convert to numerical features\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "featurized_data = hashing_tf.transform(filtered_words)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)\n",
    "\n",
    "# Show keyword importance\n",
    "rescaled_data.select(\"filtered_words\", \"features\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Influential Committers (Top Contributors & Commit Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Count commits per author\n",
    "top_committers_df = (\n",
    "    commits_df.groupBy(\"author_name\")\n",
    "    .agg(count(\"*\").alias(\"commit_count\"))\n",
    "    .orderBy(col(\"commit_count\").desc())\n",
    ")\n",
    "\n",
    "# Show top 10 committers\n",
    "top_committers_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "top_committers_pd = top_committers_df.limit(20).toPandas()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(top_committers_pd[\"author_name\"], top_committers_pd[\"commit_count\"])\n",
    "plt.xlabel(\"Committer\")\n",
    "plt.ylabel(\"Number of Commits\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 20 Most Prolific Committers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit Message Uniqueness Analysis (LSH for Similarity Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, MinHashLSH\n",
    "\n",
    "# Tokenize words from commit messages\n",
    "tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(commits_df.select(\"message\"))\n",
    "\n",
    "# Convert words into feature vectors\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=5000, minDF=5)\n",
    "cv_model = cv.fit(words_data)\n",
    "vectorized_df = cv_model.transform(words_data)\n",
    "\n",
    "# Apply LSH to detect similarity\n",
    "lsh = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", numHashTables=5)\n",
    "lsh_model = lsh.fit(vectorized_df)\n",
    "\n",
    "# Finding duplicate commit messages\n",
    "duplicates_df = lsh_model.approxSimilarityJoin(vectorized_df, vectorized_df, 0.8, distCol=\"JaccardDistance\")\n",
    "\n",
    "duplicates_df.select(\"datasetA.message\", \"datasetB.message\", \"JaccardDistance\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-runtime-0000253ceb24",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "htj2-ads-bdp on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000253ceb24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
